# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b3vlM6lQlXiYSuzoelFX_HuV7nlgWJqm
"""

pip install transformers

import pandas as pd
import numpy as np
import torch
from tqdm.auto import tqdm
import tensorflow as tf
import torch.utils.data
from torch.utils.data import TensorDataset
from transformers import TFRobertaModel, RobertaTokenizer
import joblib
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import KFold
from sklearn.metrics import confusion_matrix, precision_recall_fscore_support

from google.colab import files
upload=files.upload()

df=pd.read_excel("dataset.xlsx")
category_map={
    "Knowledge": 0,
    "Comprehension": 1,
    "Application": 2,
    "Analysis": 3,
    "Synthesis": 4,
    "Evaluation": 5
}
df["category"]=df["category"].map(category_map)
df.to_excel("dataset.xlsx",index=False)

df.head()

df['category'].value_counts()

tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

token= tokenizer.encode_plus(
    df['text'].iloc[0],
    max_length=256,
    truncation=True,
    padding='max_length',
    add_special_tokens=True,
    return_tensors='tf'
)
token.pop('token_type_ids', None)

X_input_ids= np.zeros((len(df),256))
X_attn_masks= np.zeros((len(df),256))

def generate_training_data(df, ids, masks, tokenizer):
    for i, text in tqdm(enumerate(df['text'])):
        tokenized_text = tokenizer.encode_plus(
            text,
            max_length=256,
            truncation=True,
            padding='max_length',
            add_special_tokens=True,
            return_tensors='tf'
        )
        ids[i, :] = tokenized_text['input_ids']
        masks[i, :] = tokenized_text['attention_mask']
    return ids, masks

X_input_ids, X_attn_masks = generate_training_data(df, X_input_ids, X_attn_masks, tokenizer)

labels= np.zeros((len(df),6))
labels.shape

labels[np.arange(len(df)), df['category'].values] = 1

labels

dataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels))
dataset.take(1)

def BloomsDatasetMapFunction(input_ids, attn_masks, labels):
    return {
        'input_ids': input_ids,
        'attention_mask': attn_masks
    }, labels

dataset = dataset.map(BloomsDatasetMapFunction)

dataset = dataset.shuffle(10000).batch(16, drop_remainder=True)

dataset.take(1)

p=0.8
train_size=int((len(df)//16*p))

train_dataset=dataset.take(train_size)
val_dataset=dataset.skip(train_size)
len(train_dataset),len(val_dataset)

roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
roberta_model = TFRobertaModel.from_pretrained('roberta-base')

input_ids = tf.keras.layers.Input(shape=(256,), name='input_ids', dtype='int32')
attn_masks = tf.keras.layers.Input(shape=(256,), name='attention_mask', dtype='int32')

roberta_embds = roberta_model(input_ids, attention_mask=attn_masks)[1] 
intermediate_layer = tf.keras.layers.Dense(256, activation='relu', name='roberta_intermediate_layer')(roberta_embds)
output_layer = tf.keras.layers.Dense(6, activation='softmax', name='roberta_output_layer')(intermediate_layer) 

Blooms_model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)
Blooms_model.summary()

optim = tf.keras.optimizers.legacy.Adam(learning_rate=1e-5, decay=1e-6)
loss_func = tf.keras.losses.CategoricalCrossentropy()
acc = tf.keras.metrics.CategoricalAccuracy('accuracy')

Blooms_model.compile(optimizer=optim, loss=loss_func, metrics=[acc])

from sklearn.metrics import confusion_matrix, precision_recall_fscore_support

for epoch in range(5):
    # Train the model for one epoch
    hist = Blooms_model.fit(
        train_dataset,
        validation_data=val_dataset
    )
    # Evaluate the model on the validation set
    val_loss, val_acc = Blooms_model.evaluate(val_dataset, verbose=0)
    print(f"Epoch {epoch + 1}: Val Loss: {val_loss:.3f}, Val Acc: {val_acc:.3f}")
    y_true, y_pred = [], []
    for x, y in val_dataset:
        y_true.append(tf.argmax(y, axis=1).numpy())
        y_pred.append(tf.argmax(Blooms_model.predict(x), axis=1).numpy())
# Concatenate predictions and true labels for all batches
y_true = np.concatenate(y_true)
y_pred = np.concatenate(y_pred)

# Compute confusion matrix and classification report for the entire validation set
cm = confusion_matrix(y_true, y_pred)
print("Confusion Matrix:")
print(cm)

print("Classification Report:")
print(precision_recall_fscore_support(y_true, y_pred, average='weighted'))

def classify_question(question):
    
    encoded_question = tokenizer.encode_plus(
        question,
        max_length=256,
        truncation=True,
        padding='max_length',
        return_attention_mask=True,
        return_token_type_ids=False,
        return_tensors='tf'
    )
    input_ids = encoded_question['input_ids']
    attn_mask = encoded_question['attention_mask']
    
    # Make predictions and return the predicted class name
    prediction = model.predict([input_ids, attn_mask])#testcase: model used
    predicted_class_num = tf.argmax(prediction, axis=1).numpy()[0]
    
    # Map the class number to the class name
    
    class_names = ["Knowledge", "Comprehension", "Application", "Analysis", "Synthesis", "Evaluation"]
    predicted_class_name = class_names[predicted_class_num]
    
    if question=="" or question.isspace():
      return("Please enter a valid question!!")

    else:
      return(predicted_class_name)